---
title: "X-Journalism"
output:
  html_notebook: default
  html_document: 
    toc: yes
    number_sections: yes
---

We investigate unigram terms and mutlti-word units containing the pattern `urnalis` e.g. journalism or journalist, in English Twitter. The LT dataset comprises tweets from the Twitter 1 % Streaming API. From this, all English language tweets (as identified by Twitter metadata) which contain the string `urnalis` are extracted. To exclude occurrences of twitter user handles (e.g. @cnn_journalist) from replies, user handles have been removed before pattern matching. 

*Update:* We also filter for `urnalism' (to exclude journalist etc.) and exclude Retweets from the dataset.

```{r readdata, message=FALSE}
options(stringsAsFactors = F, scipen = 999)

library(dplyr)

# read data 
csv_files <- list.files("data3", pattern = "*.tsv", full.names = T)
twitterdata <- do.call(rbind, lapply(csv_files, read.csv, sep = "\t", header = F, encoding = "UTF-8"))
colnames(twitterdata) <- c(
  "id", 
  "created_at", 
  "text", 
  "in_reply_to_status_id", 
  "in_reply_to_user_id", 
  "in_reply_to_screen_name",
  "user.id",
  "user.name",
  "user.screen_name",
  "verified",
  "followers_count",
  "friends_count",
  "location",
  "urls"
)

# convert dates
old_locale <- Sys.getlocale("LC_TIME")
invisible(Sys.setlocale("LC_TIME", "C"))
format.str <- "%a %b %d %H:%M:%S %z %Y"
twitterdata <- twitterdata %>% 
  mutate(
    date = as.POSIXct(strptime(created_at, format.str, tz = "GMT"), tz = "GMT")
  ) %>%
  mutate(
    year = format(date, "%Y")
  )
invisible(Sys.setlocale("LC_TIME", old_locale))

# filter for urnalism
twitterdata <- twitterdata[grepl("urnalism", twitterdata$text, ignore.case = T), ]

# filter for retweets 
twitterdata <- twitterdata[!grepl("^RT ", twitterdata$text, perl = T), ]

print(head(twitterdata))
```

# Unigram Frequency

We extract tweets from 2018, 2019 and 2020 along with a number of metadata.

This is how many tweets we collected.

```{r stats}
counts_per_year <- twitterdata %>%
  group_by(year) %>%
  count()
print(counts_per_year)
```

Here, we count the different word frequencies of single uigram tokens containing the pattern `urnalis`.

```{r xjournalisms, message=F}
require(quanteda)
tw_tokens <- corpus(twitterdata) %>%
  tokens() %>%
  tokens_tolower(keep_acronyms = T)
tw_dfm <- tw_tokens %>%
  tokens_keep(pattern = "urnalis", valuetype = "regex") %>%
  dfm()
token_counts <- tw_dfm %>%
  textstat_frequency()
token_counts <- token_counts[grepl("urnalism", token_counts$feature), ]
print(token_counts)
write.csv2(token_counts, file = "output/unigram.csv", fileEncoding = "UTF-8")
```

Are there any pattern matches without a j?

```{r variations, message=F}
token_counts %>%
  filter(!grepl("j", feature))
```

# Bigram frequency

We also look at frequent patterns of two consecutive words where the last word contains the pattern.

```{r mwu2}
# create tokens object
tw_tokens_sw <- tw_tokens %>%
  tokens_remove(pattern = stopwords(), padding = T)

# collocations with size 2
tw_collocations <- textstat_collocations(tw_tokens_sw, min_count = 5)
tw_collocations <- tw_collocations[grepl(".+ .*urnalism", tw_collocations$collocation, perl = T), ]
write.csv2(tw_collocations, file = "output/bigram.csv", fileEncoding = "UTF-8")

# output
print(tw_collocations)
```

# Trigram frequency

And we look at frequent patterns of three consecutive words where the last word contains the pattern.

```{r mwu3}
# collocations with size 3
tw_collocations_3 <- textstat_collocations(tw_tokens_sw, min_count = 10, size = 3)
tw_collocations_3 <- tw_collocations_3[grepl(".* .*urnalis[^\\s]*$", tw_collocations_3$collocation, perl = T), ]
tw_collocations_3$journalism <- grepl("urnalism", tw_collocations_3$collocation, perl = T)
print(tw_collocations_3)
write.csv2(tw_collocations_3, file = "output/trigram.csv", fileEncoding = "UTF-8")
```

All extracted list are exported as CSV into the `output' directory for further processing.


# Links to media outlets

We are interested in tweets linking to media outlets. For this, we identify all domains for a frequency analysis. The results are manually categorized into either media outlet or other sites.

Since there can be multiple domains in one tweet, we split first the `urls` column by space delimiter and then count the domains.

```{r extract_links}
domain_df <- twitterdata[twitterdata$urls != "" & !is.na(twitterdata$urls), c("id", "urls")]
s <- strsplit(domain_df$urls, split = " ")
domain_df <- data.frame(V1 = rep(domain_df$id, sapply(s, length)), urls = unlist(s))

# start counting
domains <- urltools::domain(domain_df$urls)
n_domains <- sort(table(domains), decreasing = T)
n_domains <- as.data.frame(n_domains)
colnames(n_domains) <- c("Domain", "Frequency")

# check output
# head(n_domains, 50)
# we can see a lot of url shorteners in the data
# View(domain_df[domains == "trib.al", ])

# so, let's expand these urls!
if (FALSE) {
  # this takes a while, so we use the saved result
  selection_idx <- domains %in% c("bit.ly", "dlvr.it", "buff.ly", "ow.ly", "ift.tt", "trib.al")
  domains_to_expand <- sample(unique(domain_df$urls[selection_idx]))
  expanded_urls <- longurl::expand_urls(domains_to_expand)
  # save(expanded_urls, file = "expanded_urls.RData")
}
# load precomputed data
load("expanded_urls.RData")
expanded_urls <- expanded_urls[!is.na(expanded_urls$expanded_url), ]
domain_df <- domain_df %>%
  left_join(expanded_urls, by = c("urls" =  "orig_url"))
idx <- is.na(domain_df$expanded_url)
domain_df[idx, "expanded_url"] <- domain_df[idx, "urls"]

# ... and count again
domains <- urltools::domain(domain_df$expanded_url)
n_domains <- sort(table(domains), decreasing = T)
n_domains <- as.data.frame(n_domains)
colnames(n_domains) <- c("Domain", "Frequency")
head(n_domains, 50)
write.csv2(n_domains, file = "domains_freq.csv", fileEncoding = "UTF-8")
```


# Sentiment Analysis 

```{r sentiment}
# export tweets to csv
write.csv(twitterdata[, c("id", "text")], file = "all_tweets.csv", fileEncoding = "UTF-8")
# run python script for sentiment and offensive language detection

# load results from python script
all_scores <- read.csv("all_tweets_scored.csv", encoding = "UTF-8")
twitterdata$sentiment <- all_scores$score
rm(all_scores)

o <- order(twitterdata$sentiment)
# most negative
most_negative <- as.data.frame(twitterdata[head(o, 25), "text"])
print(most_negative)

# most positive
most_positive <- as.data.frame(twitterdata[tail(o, 25), "text"])
print(most_positive)

# histogram of sentiment scores
hist(twitterdata$sentiment)
```

# Offensive Language Detection

... tbc